# Conference Papers

This table compiles the papers presented at the conference, including a brief description, poster images, and links to resources such as the paper, code, and video. Some papers are highlighted, best paper candidates, or award candidates.

## Icons Legend

- ğŸ… **Best Paper**: This paper has been selected as the best paper of the conference.
- ğŸ† **Award Candidate**: This paper is an award candidate.
- ğŸŒŸ **Highlighted**: This paper is featured due to its innovation or significance.

## 3D

| Paper Title | Poster | Resources | Description |
|-------------|--------|-----------|-------------|
| **VGGT: Visual Geometry Grounded Transformer** ğŸ… | ![Poster Image](./assets/poster1.png) | [ğŸ“„ Paper](https://arxiv.org/abs/2503.11651) <br> [ğŸ’» Code](https://github.com/facebookresearch/vggt) <br> [ğŸ¥ Video](https://youtu.be/7ZYwJEpCUUA) | 2D video or image to 3d scene model in seconds |
| **S3D: Sketch Driven 3D Model Generation**  | ![Poster Image](./assets/poster1.png) | [ğŸ“„ Paper] <br> [ğŸ’» Code]<br> [ğŸ¥ Video] | Generate 3D face from sketch |



## Detection, Tracking and Re-identification

| Paper Title | Poster | Resources | Description |
|-------------|--------|-----------|-------------|
| **AG-VPReID 2025: The 2nd Aerial-Ground Person ReID Challenge**  | ![Poster Image](./assets/poster1.png) | [ğŸ“„ Paper](https://arxiv.org/abs/2503.08121) <br> [ğŸ’» Code](https://github.com/agvpreid25/AG-VPReID) <br> [ğŸ¥ Video](https://youtu.be/00DhDxvwbiY)| Aerial-Ground view re-identification, from 8m to 120m height. Can be used for G2G, G2A, A2G |
| **CaMuViD: Calibration-Free Multi-View Detection** | ![Poster Image](./assets/poster2.png) | [Paper](https://openaccess.thecvf.com/content/CVPR2025/papers/Daryani_CaMuViD_Calibration-Free_Multi-View_Detection_CVPR_2025_paper.pdf) <br> [Code](https://github.com/amiretefaghi/CaMuViD) <br> [Video](https://youtu.be/LJzFkKqth6g) | This paper presents [short description of the paper]. Award candidate. |
| **MambaVLT: Time-Evolving Multimodal State Space Model for Vision-Language Tracking**  | ![Poster Image](./assets/poster1.png) | [ğŸ“„ Paper] <br> [ğŸ’» Code]<br> [ğŸ¥ Video] | Vision Language Tracker using State Space model Mamba which is faster than transformers |

## Video Understanding / Summary

| Paper Title | Poster | Resources | Description |
|-------------|--------|-----------|-------------|
| **VideoICL: Confidence-based Iterative In-context Learning for Out-of-Distribution Video Understanding**  | ![Poster Image](./assets/poster1.png) | [ğŸ“„ Paper](https://arxiv.org/abs/2412.02186) <br> [ğŸ’» Code](https://github.com/KangsanKim07/VideoICL)<br> [ğŸ¥ Video](https://youtu.be/00DhDxvwbiY)| In Context Learning approach for video understanding. |
| **STPro: Spatial and Temporal Progressive Learning for Weakly Supervised Spatio-Temporal Grounding**  | ![Poster Image](./assets/poster1.png) | [ğŸ“„ Paper] <br> [ğŸ’» Code]<br> [ğŸ¥ Video] | Process Spatial and Temporal separately for better understanding |
| **EdgeVidSum: Real-Time Personalized Video Summarization at the Edge**  | ![Poster Image](./assets/poster1.png) | [ğŸ“„ Paper] <br> [ğŸ’» Code]<br> [ğŸ¥ Video] | Video Summarisation on Edge |
| **Open World Scene Graph Generation using VLM**  | ![Poster Image](./assets/poster1.png) | [ğŸ“„ Paper] <br> [ğŸ’» Code]<br> [ğŸ¥ Video] | A trainingless VLM and grounding DINO appoach to generate scene graph |

## Foundation Models and Encoders

| Paper Title | Poster | Resources | Description |
|-------------|--------|-----------|-------------|
| **Token Cropr: Faster ViTs for Quite a Few Tasks**  | ![Poster Image](./assets/poster1.png) | [ğŸ“„ Paper] <br> [ğŸ’» Code]<br> [ğŸ¥ Video] | Faster ViT |
| **Perception Encoder: The best visual embeddings are not at the output of the network**  | ![Poster Image](./assets/poster1.png) | [ğŸ“„ Paper] <br> [ğŸ’» Code]<br> [ğŸ¥ Video] | New CLIP by meta, surpass CLIP for image tasks, with limited video capability due to poor temporal knowledge |
| **REN: Fast and Efficient Region Encodings from Patch-Based Image Encoders**  | ![Poster Image](./assets/poster1.png) | [ğŸ“„ Paper] <br> [ğŸ’» Code]<br> [ğŸ¥ Video] | High Speed and efficient image region encoders |
| **CAV-MAE Sync: Improving Contrastive Audio-Visual Mask Autoencoders via Fine-Grained Alignment**  | ![Poster Image](./assets/poster1.png) | [ğŸ“„ Paper] <br> [ğŸ’» Code]<br> [ğŸ¥ Video] | Better audio visual representation |

## Video Grounding / Search

| Paper Title | Poster | Resources | Description |
|-------------|--------|-----------|-------------|
| **VideoGEM: Training-free Action Grounding in Videos**  | ![Poster Image](./assets/poster1.png) | [ğŸ“„ Paper] <br> [ğŸ’» Code]<br> [ğŸ¥ Video] | Perform prompt decomposition and use static and dynamic weighting GEM module to do action recognition |
| **Re-thinking Temporal Search for Long-Form Video Understanding**  | ![Poster Image](./assets/poster1.png) | [ğŸ“„ Paper] <br> [ğŸ’» Code]<br> [ğŸ¥ Video] | Smart frame selector leveraging on cue and target object search using additional module for zero-shot detection and scoring to identify possible locality before zooming into segment for in depth analysis |
| **RELOCATE: A Simple Training-Free Baseline for Visual Query Localization Using Region-Based Representations**  | ![Poster Image](./assets/poster1.png) | [ğŸ“„ Paper] <br> [ğŸ’» Code]<br> [ğŸ¥ Video] | Using SAM and DINO to perform image-based object query on videos |
| **BIMBA: Selective Scan Compression for Long Range Video QA**  | ![Poster Image](./assets/poster1.png) | [ğŸ“„ Paper] <br> [ğŸ’» Code]<br> [ğŸ¥ Video] | Uses MAMBA for video search and scanning of spatial-temporal tokens. Said to be more efficient than transformer based architectures |

## Temporal and Motion in VLM

| Paper Title | Poster | Resources | Description |
|-------------|--------|-----------|-------------|
| **Temporal Alignment-Free Video Matching for Few-shot Action Recognition**  | ![Poster Image](./assets/poster1.png) | [ğŸ“„ Paper] <br> [ğŸ’» Code]<br> [ğŸ¥ Video] | Few shot action recognition for unseen or OOD actions via clustering |
| **MotionBench: Benchmarking and Improving Fine-grained Video Motion Understanding for Vision Language Models**  | ![Poster Image](./assets/poster1.png) | [ğŸ“„ Paper] <br> [ğŸ’» Code]<br> [ğŸ¥ Video] | A motion focused dataset and benchmark for VLM, including novel TE-Fusion module to enhance pre-trained VLM's with better finegrained motion understanding |
| **Context-Enhanced Memory-Refined Transformer for Online Action Detection**  | ![Poster Image](./assets/poster1.png) | [ğŸ“„ Paper] <br> [ğŸ’» Code]<br> [ğŸ¥ Video] | Real Time Action detection |

## Pre-training, Finetuning and Visualization

| Paper Title | Poster | Resources | Description |
|-------------|--------|-----------|-------------|
| **Prompt-CAM: Making Vision Transformers Interpretable for Fine-Grained Analysis**  | ![Poster Image](./assets/poster1.png) | [ğŸ“„ Paper] <br> [ğŸ’» Code]<br> [ğŸ¥ Video] | A simple finetune model to visualize a pre-trained model's attention behaviour |

## Segmentation

| Paper Title | Poster | Resources | Description |
|-------------|--------|-----------|-------------|
| **CALICO: Part-Focused Semantic Co-Segmentation with Large Vision-Language Models**  | ![Poster Image](./assets/poster1.png) | [ğŸ“„ Paper] <br> [ğŸ’» Code]<br> [ğŸ¥ Video] | Part Segmentation Model for human/animal parts |
| **EdgeTAM: On-Device Track Anything Model**  | ![Poster Image](./assets/poster1.png) | [ğŸ“„ Paper] <br> [ğŸ’» Code]<br> [ğŸ¥ Video] | SAM2 Tracking on edge with 20x speed up on edge device|


## Video Anomaly Detection

| Paper Title | Poster | Resources | Description |
|-------------|--------|-----------|-------------|
| **Track Any Anomalous Object: A Granular Video Anomaly Detection Pipeline**  | ![Poster Image](./assets/poster1.png) | [ğŸ“„ Paper] <br> [ğŸ’» Code]<br> [ğŸ¥ Video] | Pixel Level anomaly detection and tracking for more fine-grained anomaly detection compared to scene or object anomaly |
| **VERA: Explainable Video Anomaly Detection via Verbalized Learning of Vision-Language Models**  | ![Poster Image](./assets/poster1.png) | [ğŸ“„ Paper] <br> [ğŸ’» Code]<br> [ğŸ¥ Video] | Verbalized Learning approach for VLM to detect anomaly using learnable guide prompts. The anomaly detection module is a small module based on the VLM to learn a set of guiding questions based on the input prompt to help the VLM verbalize and generalize better. |
| **AssistPDA: An Online Video Surveillance Assistant for Video Anomaly Prediction, Detection, and Analysis**  | ![Poster Image](./assets/poster1.png) | [ğŸ“„ Paper] <br> [ğŸ’» Code]<br> [ğŸ¥ Video] | Live VLM based anomaly and crime detection by NUS. No code yet |


## Audio Video Fusion

| Paper Title | Poster | Resources | Description |
|-------------|--------|-----------|-------------|
| **Adapting to the Unknown: Training-Free Audio-Visual Event Perception with Dynamic Thresholds**  | ![Poster Image](./assets/poster1.png) | [ğŸ“„ Paper] <br> [ğŸ’» Code]<br> [ğŸ¥ Video] | Audio Visual Event understanding to boost foundation models |
| **UWAV: Uncertainty-weighted Weakly-supervised Audio-Visual Video Parsing**  | ![Poster Image](./assets/poster1.png) | [ğŸ“„ Paper] <br> [ğŸ’» Code]<br> [ğŸ¥ Video] | AVVP approach that overcomes weakness in inter-segment dependencies and achieve SOTA |
| **LongVALE: Vision-Audio-Language-Event Benchmark Towards Time-Aware Omni-Modal Perception of Long Videos**  | ![Poster Image](./assets/poster1.png) | [ğŸ“„ Paper] <br> [ğŸ’» Code]<br> [ğŸ¥ Video] | A multimodal vision audio language dataset for temporal understanding, and audio-visual event captioning/detection. Also has their own model with training recipe which has good performance for temporal events. |
| **LiveCC: Learning Video LLM with streaming speech transcription at scale**  | ![Poster Image](./assets/poster1.png) | [ğŸ“„ Paper] <br> [ğŸ’» Code]<br> [ğŸ¥ Video] | Live closed-captioning / commentary of streaming video events in real time |

## Image Restoration / Enhancement

| Paper Title | Poster | Resources | Description |
|-------------|--------|-----------|-------------|
| **HVI: A New Color Space for Low-light Image Enhancement**  | ![Poster Image](./assets/poster1.png) | [ğŸ“„ Paper] <br> [ğŸ’» Code]<br> [ğŸ¥ Video] | Real time deblurring, denoising, low-light and overexposure enhancement |

## Traffic Understanding

| Paper Title | Poster | Resources | Description |
|-------------|--------|-----------|-------------|
| **RoadSocial: A Diverse VideoQA Dataset and Benchmark for Road Event Understanding from Social Video Narratives**  | ![Poster Image](./assets/poster1.png) | [ğŸ“„ Paper] <br> [ğŸ’» Code]<br> [ğŸ¥ Video] | A dataset collected from traffic videos all over the world, including violation videos for traffic event QA, captioning and violation detection etc. The team finetuned their own Llava OV model on the dataset which can be tested |
