# Conference Papers

This table compiles the papers presented at the conference, including a brief description, poster images, and links to resources such as the paper, code, and video. Some papers are highlighted, best paper candidates, or award candidates.

## Icons Legend

- 🏅 **Best Paper**: This paper has been selected as the best paper of the conference.
- 🏆 **Award Candidate**: This paper is an award candidate.
- 🌟 **Highlighted**: This paper is featured due to its innovation or significance.

## 3D

| Paper Title | Poster | Resources | Description |
|-------------|--------|-----------|-------------|
| **VGGT: Visual Geometry Grounded Transformer** 🏅 | ![Poster Image](./assets/poster1.png) | [📄 Paper](https://arxiv.org/abs/2503.11651) <br> [💻 Code](https://github.com/facebookresearch/vggt) <br> [🎥 Video](https://youtu.be/7ZYwJEpCUUA) | 2D video or image to 3d scene model in seconds |
| **Paper 2 Title** <br> ![Award Candidate](./assets/award_candidate.png) | ![Poster Image](./assets/poster2.png) | [Paper](https://example.com/paper2) <br> [Code](https://example.com/code2) <br> [Video](https://example.com/video2) | This paper presents [short description of the paper]. Award candidate. |
| **Paper 3 Title** <br> ![Highlighted](./assets/highlighted.png) | ![Poster Image](./assets/poster3.png) | [Paper](https://example.com/paper3) <br> [Code](https://example.com/code3) <br> [Video](https://example.com/video3) | A detailed analysis of [short description]. Highlighted paper. |


## Detection, Tracking and Re-identification

| Paper Title | Poster | Resources | Description |
|-------------|--------|-----------|-------------|
| **AG-VPReID 2025: The 2nd Aerial-Ground Person ReID Challenge**  | ![Poster Image](./assets/poster1.png) | [📄 Paper](https://arxiv.org/abs/2503.08121) <br> [💻 Code](https://github.com/agvpreid25/AG-VPReID) <br> [🎥 Video](https://youtu.be/00DhDxvwbiY)| Aerial-Ground view re-identification, from 8m to 120m height. Can be used for G2G, G2A, A2G |
| **CaMuViD: Calibration-Free Multi-View Detection** | ![Poster Image](./assets/poster2.png) | [Paper](https://openaccess.thecvf.com/content/CVPR2025/papers/Daryani_CaMuViD_Calibration-Free_Multi-View_Detection_CVPR_2025_paper.pdf) <br> [Code](https://github.com/amiretefaghi/CaMuViD) <br> [Video](https://youtu.be/LJzFkKqth6g) | This paper presents [short description of the paper]. Award candidate. |
| **MambaVLT: Time-Evolving Multimodal State Space Model for Vision-Language Tracking**  | ![Poster Image](./assets/poster1.png) | [📄 Paper] <br> [💻 Code]<br> [🎥 Video] | Vision Language Tracker using State Space model Mamba which is faster than transformers |

## Video Understanding / Summary

| Paper Title | Poster | Resources | Description |
|-------------|--------|-----------|-------------|
| **VideoICL: Confidence-based Iterative In-context Learning for Out-of-Distribution Video Understanding**  | ![Poster Image](./assets/poster1.png) | [📄 Paper](https://arxiv.org/abs/2412.02186) <br> [💻 Code](https://github.com/KangsanKim07/VideoICL)<br> [🎥 Video](https://youtu.be/00DhDxvwbiY)| In Context Learning approach for video understanding. |
| **STPro: Spatial and Temporal Progressive Learning for Weakly Supervised Spatio-Temporal Grounding**  | ![Poster Image](./assets/poster1.png) | [📄 Paper] <br> [💻 Code]<br> [🎥 Video] | Process Spatial and Temporal separately for better understanding |
| **EdgeVidSum: Real-Time Personalized Video Summarization at the Edge**  | ![Poster Image](./assets/poster1.png) | [📄 Paper] <br> [💻 Code]<br> [🎥 Video] | Video Summarisation on Edge |

## Foundation Models

| Paper Title | Poster | Resources | Description |
|-------------|--------|-----------|-------------|
| **Token Cropr: Faster ViTs for Quite a Few Tasks**  | ![Poster Image](./assets/poster1.png) | [📄 Paper] <br> [💻 Code]<br> [🎥 Video] | Faster ViT |
| **Perception Encoder: The best visual embeddings are not at the output of the network**  | ![Poster Image](./assets/poster1.png) | [📄 Paper] <br> [💻 Code]<br> [🎥 Video] | New CLIP by meta, surpass CLIP for image tasks, with limited video capability due to poor temporal knowledge |

## Video Grounding / Search

| Paper Title | Poster | Resources | Description |
|-------------|--------|-----------|-------------|
| **VideoGEM: Training-free Action Grounding in Videos**  | ![Poster Image](./assets/poster1.png) | [📄 Paper] <br> [💻 Code]<br> [🎥 Video] | Perform prompt decomposition and use static and dynamic weighting GEM module to do action recognition |
| **Re-thinking Temporal Search for Long-Form Video Understanding**  | ![Poster Image](./assets/poster1.png) | [📄 Paper] <br> [💻 Code]<br> [🎥 Video] | Smart frame selector leveraging on cue and target object search using additional module for zero-shot detection and scoring to identify possible locality before zooming into segment for in depth analysis |


## Temporal and Motion in VLM

| Paper Title | Poster | Resources | Description |
|-------------|--------|-----------|-------------|
| **Temporal Alignment-Free Video Matching for Few-shot Action Recognition**  | ![Poster Image](./assets/poster1.png) | [📄 Paper] <br> [💻 Code]<br> [🎥 Video] | Few shot action recognition for unseen or OOD actions via clustering |
| **MotionBench: Benchmarking and Improving Fine-grained Video Motion Understanding for Vision Language Models**  | ![Poster Image](./assets/poster1.png) | [📄 Paper] <br> [💻 Code]<br> [🎥 Video] | A motion focused dataset and benchmark for VLM, including novel TE-Fusion module to enhance pre-trained VLM's with better finegrained motion understanding |
| **Context-Enhanced Memory-Refined Transformer for Online Action Detection**  | ![Poster Image](./assets/poster1.png) | [📄 Paper] <br> [💻 Code]<br> [🎥 Video] | Real Time Action detection |

## Pre-training, Finetuning and Visualization

| Paper Title | Poster | Resources | Description |
|-------------|--------|-----------|-------------|
| **Prompt-CAM: Making Vision Transformers Interpretable for Fine-Grained Analysis**  | ![Poster Image](./assets/poster1.png) | [📄 Paper] <br> [💻 Code]<br> [🎥 Video] | A simple finetune model to visualize a pre-trained model's attention behaviour |

## Segmentation

| Paper Title | Poster | Resources | Description |
|-------------|--------|-----------|-------------|
| **CALICO: Part-Focused Semantic Co-Segmentation with Large Vision-Language Models**  | ![Poster Image](./assets/poster1.png) | [📄 Paper] <br> [💻 Code]<br> [🎥 Video] | Part Segmentation Model for human/animal parts |
| **EdgeTAM: On-Device Track Anything Model**  | ![Poster Image](./assets/poster1.png) | [📄 Paper] <br> [💻 Code]<br> [🎥 Video] | SAM2 Tracking on edge with 20x speed up on edge device|

## Video Anomaly Detection

| Paper Title | Poster | Resources | Description |
|-------------|--------|-----------|-------------|
| **Track Any Anomalous Object: A Granular Video Anomaly Detection Pipeline**  | ![Poster Image](./assets/poster1.png) | [📄 Paper] <br> [💻 Code]<br> [🎥 Video] | Pixel Level anomaly detection and tracking for more fine-grained anomaly detection compared to scene or object anomaly |
| **VERA: Explainable Video Anomaly Detection via Verbalized Learning of Vision-Language Models**  | ![Poster Image](./assets/poster1.png) | [📄 Paper] <br> [💻 Code]<br> [🎥 Video] | Verbalized Learning approach for VLM to detect anomaly using learnable guide prompts |

## Audio Video Fusion

| Paper Title | Poster | Resources | Description |
|-------------|--------|-----------|-------------|
| **Adapting to the Unknown: Training-Free Audio-Visual Event Perception with Dynamic Thresholds**  | ![Poster Image](./assets/poster1.png) | [📄 Paper] <br> [💻 Code]<br> [🎥 Video] | Audio Visual Event understanding to boost foundation models |

## Image Restoration / Enhancement

| Paper Title | Poster | Resources | Description |
|-------------|--------|-----------|-------------|
| **HVI: A New Color Space for Low-light Image Enhancement**  | ![Poster Image](./assets/poster1.png) | [📄 Paper] <br> [💻 Code]<br> [🎥 Video] | Real time deblurring, denoising, low-light and overexposure enhancement |
